{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERPLEXITY=5\n",
    "g_kernel=1\n",
    "EPOCHS=2000\n",
    "LR=200\n",
    "MOMENTUM=0.99\n",
    "\n",
    "def getKey(item):\n",
    "    return item[1]\n",
    "\n",
    "#compute the distance between the neighboors of x1 and return a list of the k neghboors\n",
    "#where k is the complexity\n",
    "def k_neighbours(x,x1_index,p_or_q='p'):\n",
    "    x1=x[x1_index]\n",
    "    list_k_neighbours=[]\n",
    "    for i in range(x.shape[0]):\n",
    "        if i!=x1_index:\n",
    "            xi=x[i]\n",
    "            if p_or_q=='p':\n",
    "                distance=np.exp(-np.linalg.norm(x1-xi)**2/(2*g_kernel**2))\n",
    "            else:\n",
    "                distance=(1+np.linalg.norm(x1-xi)**2)**-1\n",
    "            list_k_neighbours.append([i,distance])\n",
    "    \n",
    "    list_k_neighbours=sorted(list_k_neighbours,key=getKey)\n",
    "    return list_k_neighbours[:PERPLEXITY]\n",
    "\n",
    "#compute the similarity pij between two xi,xj in the original space\n",
    "#divide the distance between xi,xj by the sum of the distances of the k_neightbours where k is the complexity\n",
    "def compute_pij(x,x1_index,x2_index):\n",
    "    x1=x[x1_index]\n",
    "    x2=x[x2_index]\n",
    "    # num=(1+np.linalg.norm(x1-x2)**2)**(-1)/(2*g_kernel**2))\n",
    "    num=np.exp(-np.linalg.norm(x1-x2)**2)/(2*g_kernel**2)\n",
    "    denom=0\n",
    "    list_k_neighbours=k_neighbours(x,x1_index,'p')\n",
    "    for i in list_k_neighbours:\n",
    "        denom+=i[1]\n",
    "    return num/denom\n",
    "\n",
    "\n",
    "#compute the table p of the xij in the original space\n",
    "def compute_p(x):\n",
    "    table=np.zeros((x.shape[0],x.shape[0]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[0]):\n",
    "            if i!=j:\n",
    "                pij=compute_pij(x,i,j)\n",
    "                pji=compute_pij(x,j,i)\n",
    "                table[i,j]=(pij+pji)/(2*x.shape[0])\n",
    "                # table[i,j]=pij\n",
    "    return table\n",
    "\n",
    "#compute the similarity qij between two yi,yj in the new space\n",
    "#divide the distance between yi,yj by the sum of the distances of the k_neightbours where k is the complexity\n",
    "def compute_qij(y,y1_index,y2_index):\n",
    "    y1=y[y1_index]\n",
    "    y2=y[y2_index]\n",
    "    num=(1+np.linalg.norm(y1-y2)**2)**(-1)\n",
    "    denom=0\n",
    "    for i in k_neighbours(y,y1_index,'q'):\n",
    "        denom+=i[1]\n",
    "    return num/denom\n",
    "\n",
    "#compute the table q of the yij in the new space\n",
    "def compute_q(y):\n",
    "    table=np.zeros((y.shape[0],y.shape[0]))\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[0]):\n",
    "            if i!=j:\n",
    "                qij=compute_qij(y,i,j)\n",
    "                table[i,j]=qij\n",
    "    return table\n",
    "\n",
    "#compute the erros between the 2 distributions using the KL-divergence\n",
    "def kl_divergence(p,q):\n",
    "    total=0\n",
    "    for i in range(p.shape[0]):\n",
    "        for j in range(q.shape[0]):\n",
    "            if q[i,j]!=0 and p[i,j]!=0:\n",
    "                total+=p[i,j]*np.log(p[i,j]/q[i,j])\n",
    "    return total\n",
    "\n",
    "#apply gradient descent to lower the KL-divergence\n",
    "#added momentum increase the speed\n",
    "def gradient_descent(p,q,y):\n",
    "    history=np.zeros((p.shape[0],2,y.shape[1]))\n",
    "    for iter in range(EPOCHS):\n",
    "        for i in range(y.shape[0]):\n",
    "            sum_value=0\n",
    "            for j in range(y.shape[0]):\n",
    "                sum_value+=((y[i]-y[j])*(p[i,j]-q[i,j])*(1+np.linalg.norm(y[i]-y[j]**2))**-1)\n",
    "            y[i]-=4*LR*sum_value+MOMENTUM*(history[i,1]-history[i,0])\n",
    "            history[i,0]=history[i,1]\n",
    "            history[i,1]=y[i]\n",
    "        if iter%100==0:\n",
    "            q=compute_q(y)\n",
    "            print(kl_divergence(p,q)) \n",
    "    y-=np.mean(y)\n",
    "    y/=np.std(y)\n",
    "    return y\n",
    "\n",
    "\n",
    "def tsne():\n",
    "    #I choose a dataset with two well separated part\n",
    "    x=np.random.rand(10,3)\n",
    "    x=np.tile(x,(2,1))\n",
    "    x[:10]*=0.1\n",
    "    color=['blue']*10+['red']*10\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x[:,0],x[:,1],x[:,2],color=color)\n",
    "    plt.show()\n",
    "\n",
    "    table_p=compute_p(x)\n",
    "\n",
    "    #Probably not the right way to initialize the new space y\n",
    "    y=x.dot(np.random.rand(x.shape[1],2))\n",
    "    y-=np.mean(y)\n",
    "    y/=np.std(y)\n",
    "    table_q=compute_q(y)\n",
    "    y=gradient_descent(table_p,table_q,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
