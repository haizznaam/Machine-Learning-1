{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem 1**: \n",
    "Normally, building Ensemble model will be based on Decision Tree. What are the reasons for this,\n",
    "please explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "\n",
    "There are two main reasons why ensemble models are often built based on decision trees:\n",
    "\n",
    "1. **Reduce Variance:** Decision trees can be prone to variance, meaning small changes in the training data can lead to significantly different trees. Ensembling multiple trees averages out these variations, leading to a more robust and generalizable model.\n",
    "2. **Flexibility:** Decision trees can capture complex relationships in data, but a single tree might not capture all the intricacies. By combining multiple trees, you can leverage the flexibility of decision trees while reducing the risk of overfitting to the specific training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem 2**: \n",
    " Why do we use Decision Tree as weak learner when constructing Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "\n",
    "There are two main reasons why we use Decision Tree as weak learner when constructing Gradient Boosting:\n",
    "\n",
    "1. **Speed:** Decision trees are relatively fast to train, allowing you to build many of them efficiently. This is crucial because Gradient Boosting relies on adding many weak learners sequentially.\n",
    "2. **Simplicity:** Their interpretability helps each tree focus on correcting the errors of the previous one, making the boosting process more efficient.\n",
    "\n",
    "Even though they might not be the most powerful learners individually (weak), their simplicity and speed make them ideal building blocks for strong ensemble models in Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem 3**: \n",
    "Re-construct the loss of GBDT, visualize if you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "The loss function used in GBDT is often related to the residuals or errors of the model. The loss function typically used in GBDT is the L2 loss function, also known as the **Mean Squared Error (MSE).**\n",
    "\n",
    "Structure of the loss function used in GBDT:\n",
    "\n",
    "**1. Initialized model**: $\\displaystyle F_0(x) = arg min_{\\gamma} \\sum^n_{i=1} L(y_i, \\gamma)$\n",
    ", where $L$ is the loss function\n",
    "\n",
    "**2. Iteratively build the model**:\n",
    "- For $m = 1$ to $M$:\n",
    "    \n",
    "    - Compute the negative gradient of the loss function with respect to the previous model's output :\n",
    "    $$r_{im} = -[{\\frac{\\partial L (y_i, F(x_i))}{\\partial F(x_i)}}] F(x) = F_{m-1}(x)$$\n",
    "\n",
    "    - Fit a base learner (typically a decision tree) to the negative gradient $r_{im}$. This tree will predict the residual errors.\n",
    "\n",
    "    - Compute the optimal step size $\\gamma_m$ to minimize the loss function: \n",
    "    $$\\gamma_m = argmin_{\\gamma} \\sum^n_{i=1} L(y_i, F_{m-1} (x_i) + \\gamma h_m (x_i))$$\n",
    "\n",
    "    - Update the model: $F_m(x) = F_{m-1} (x) + \\gamma_m h_m(x)$, where $h_m(x)$ is the newly added decision tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem 4**: \n",
    "Implement GBDT, you can use all weak learner from SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, lr = 0.1, n_estimators = 25, base_learner = DecisionTreeRegressor):\n",
    "        self.lr = lr\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_learner = base_learner\n",
    "\n",
    "    def fit(self, X, y , **params):\n",
    "        self.base_models = []\n",
    "\n",
    "        # initial the first base model with a constant value\n",
    "        f0 = np.full(shape = y.shape, fill_value= 0.0)\n",
    "\n",
    "        # update the model\n",
    "        Fm = f0\n",
    "\n",
    "        # create a subplot for each step prediction\n",
    "        fig, axes = plt.subplots(5, 5, figsize = (10, 10))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i in range(0, self.n_estimators):\n",
    "            # compute pseudo- residuals (gradient of MSE-loss)\n",
    "            r_i = y - Fm\n",
    "\n",
    "            # base learner\n",
    "            h_i = self.base_learner(**params)\n",
    "            h_i.fit(X, r_i)\n",
    "            self.base_models.append(h_i)\n",
    "\n",
    "            # update the model\n",
    "            Fm = Fm + self.lr * h_i.predict(X)\n",
    "\n",
    "            # plotting after prediction\n",
    "            axes[i].plot(y, \".\")\n",
    "            axes[i].plot(Fm, \".\")\n",
    "            axes[i].set_title(str(i))\n",
    "            axes[i].axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        return Fm\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.array([])\n",
    "\n",
    "        for h_i in self.base_models:\n",
    "            update = self.lr * h_i.predict(X)\n",
    "            \n",
    "            if not y_pred.any():\n",
    "                y_pred = update # pred.any() is False at the beginning\n",
    "            else:\n",
    "                y_pred += update\n",
    "        \n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
